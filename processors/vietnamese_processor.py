import os
import pandas as pd
import emoji
import regex as re
from vncorenlp import VnCoreNLP
from sklearn.base import BaseEstimator, TransformerMixin

# VietnameseToneNormalizer : Chuáº©n hÃ³a Unicode (vÃ­ dá»¥: lá»±Æ¡ng=> lÆ°á»£ng, thá»ai mÃ¡i=> thoáº£i mÃ¡i).
class VietnameseToneNormalizer:
    VOWELS_TABLE = [
        ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a'],
        ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],
        ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],
        ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],
        ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],
        ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],
        ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],
        ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],
        ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],
        ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],
        ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],
        ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y']
    ]
    
    VOWELS_TO_IDS = {
        'a': (0, 0), 'Ã ': (0, 1), 'Ã¡': (0, 2), 'áº£': (0, 3), 'Ã£': (0, 4), 'áº¡': (0, 5), 
        'Äƒ': (1, 0), 'áº±': (1, 1), 'áº¯': (1, 2), 'áº³': (1, 3), 'áºµ': (1, 4), 'áº·': (1, 5), 
        'Ã¢': (2, 0), 'áº§': (2, 1), 'áº¥': (2, 2), 'áº©': (2, 3), 'áº«': (2, 4), 'áº­': (2, 5), 
        'e': (3, 0), 'Ã¨': (3, 1), 'Ã©': (3, 2), 'áº»': (3, 3), 'áº½': (3, 4), 'áº¹': (3, 5), 
        'Ãª': (4, 0), 'á»': (4, 1), 'áº¿': (4, 2), 'á»ƒ': (4, 3), 'á»…': (4, 4), 'á»‡': (4, 5), 
        'i': (5, 0), 'Ã¬': (5, 1), 'Ã­': (5, 2), 'á»‰': (5, 3), 'Ä©': (5, 4), 'á»‹': (5, 5), 
        'o': (6, 0), 'Ã²': (6, 1), 'Ã³': (6, 2), 'á»': (6, 3), 'Ãµ': (6, 4), 'á»': (6, 5), 
        'Ã´': (7, 0), 'á»“': (7, 1), 'á»‘': (7, 2), 'á»•': (7, 3), 'á»—': (7, 4), 'á»™': (7, 5), 
        'Æ¡': (8, 0), 'á»': (8, 1), 'á»›': (8, 2), 'á»Ÿ': (8, 3), 'á»¡': (8, 4), 'á»£': (8, 5), 
        'u': (9, 0), 'Ã¹': (9, 1), 'Ãº': (9, 2), 'á»§': (9, 3), 'Å©': (9, 4), 'á»¥': (9, 5), 
        'Æ°': (10, 0), 'á»«': (10, 1), 'á»©': (10, 2), 'á»­': (10, 3), 'á»¯': (10, 4), 'á»±': (10, 5), 
        'y': (11, 0), 'á»³': (11, 1), 'Ã½': (11, 2), 'á»·': (11, 3), 'á»¹': (11, 4), 'á»µ': (11, 5)
    }
    
    # Báº£ng thay tháº¿ nhanh theo khuyáº¿n nghá»‹ cá»§a VinAI cho cá»¥m 2 nguyÃªn Ã¢m Ä‘áº·c biá»‡t.
    VINAI_NORMALIZED_TONE = {
        'Ã²a': 'oÃ ', 'Ã’a': 'OÃ ', 'Ã’A': 'OÃ€', 
        'Ã³a': 'oÃ¡', 'Ã“a': 'OÃ¡', 'Ã“A': 'OÃ', 
        'á»a': 'oáº£', 'á»Ža': 'Oáº£', 'á»ŽA': 'Oáº¢',
        'Ãµa': 'oÃ£', 'Ã•a': 'OÃ£', 'Ã•A': 'OÃƒ',
        'á»a': 'oáº¡', 'á»Œa': 'Oáº¡', 'á»ŒA': 'Oáº ',
        'Ã²e': 'oÃ¨', 'Ã’e': 'OÃ¨', 'Ã’E': 'OÃˆ',
        'Ã³e': 'oÃ©', 'Ã“e': 'OÃ©', 'Ã“E': 'OÃ‰',
        'á»e': 'oáº»', 'á»Že': 'Oáº»', 'á»ŽE': 'Oáºº',
        'Ãµe': 'oáº½', 'Ã•e': 'Oáº½', 'Ã•E': 'Oáº¼',
        'á»e': 'oáº¹', 'á»Œe': 'Oáº¹', 'á»ŒE': 'Oáº¸',
        'Ã¹y': 'uá»³', 'Ã™y': 'Uá»³', 'Ã™Y': 'Uá»²',
        'Ãºy': 'uÃ½', 'Ãšy': 'UÃ½', 'ÃšY': 'UÃ',
        'á»§y': 'uá»·', 'á»¦y': 'Uá»·', 'á»¦Y': 'Uá»¶',
        'Å©y': 'uá»¹', 'Å¨y': 'Uá»¹', 'Å¨Y': 'Uá»¸',
        'á»¥y': 'uá»µ', 'á»¤y': 'Uá»µ', 'á»¤Y': 'Uá»´',
    }


    @staticmethod
    def normalize_unicode(text):
        char1252 = r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'
        charutf8 = r'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»Ž|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»ž|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())
    
    
    @staticmethod
    def normalize_sentence_typing(text, vinai_normalization=False):
        # https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
        if vinai_normalization: # Just simply replace the wrong tone with the correct one defined by VinAI
            for wrong, correct in VietnameseToneNormalizer.VINAI_NORMALIZED_TONE.items():
                text = text.replace(wrong, correct)
            return text.strip()
        
        # Or you can use this algorithm developed by Behitek to normalize Vietnamese typing in a sentence 
        words = text.strip().split()
        for index, word in enumerate(words):
            cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
            if len(cw) == 3: cw[1] = VietnameseToneNormalizer.normalize_word_typing(cw[1])
            words[index] = ''.join(cw)
        return ' '.join(words)
    
     
    @staticmethod
    def normalize_word_typing(word):
        if not VietnameseToneNormalizer.is_valid_vietnamese_word(word): return word
        chars, vowel_indexes = list(word), []
        qu_or_gi, tonal_mark = False, 0
        
        for index, char in enumerate(chars):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            row, col = VietnameseToneNormalizer.VOWELS_TO_IDS[char]
            if index > 0 and (row, chars[index - 1]) in [(9, 'q'), (5, 'g')]:
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                qu_or_gi = True
                
            if not qu_or_gi or index != 1: vowel_indexes.append(index)
            if col != 0:
                tonal_mark = col
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                
        if len(vowel_indexes) < 2:
            if qu_or_gi:
                index = 1 if len(chars) == 2 else 2
                if chars[index] in VietnameseToneNormalizer.VOWELS_TO_IDS:
                    row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
                    chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                else: chars[1] = VietnameseToneNormalizer.VOWELS_TABLE[5 if chars[1] == 'i' else 9][tonal_mark]
                return ''.join(chars)
            return word
        
        for index in vowel_indexes:
            row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
            if row in [4, 8]: # Ãª, Æ¡
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                return ''.join(chars)
            
        index = vowel_indexes[0 if len(vowel_indexes) == 2 and vowel_indexes[-1] == len(chars) - 1 else 1] 
        row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
        chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
        return ''.join(chars)
    
    
    @staticmethod
    def is_valid_vietnamese_word(word):
        vowel_indexes = -1 
        for index, char in enumerate(word):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            if vowel_indexes in [-1, index - 1]: vowel_indexes = index
            else: return False
        return True

"""
VietnameseTextCleaner lÃ  cÃ´ng cá»¥ lÃ m sáº¡ch vÄƒn báº£n Ä‘Æ¡n giáº£n dá»±a trÃªn regex Ä‘á»ƒ xÃ³a HTML, Emoji, 
URL, Email, Sá»‘ Ä‘iá»‡n thoáº¡i, Hashtag vÃ  cÃ¡c kÃ½ tá»± khÃ´ng cáº§n thiáº¿t khÃ¡c.
"""
class VietnameseTextCleaner:
    VN_CHARS = 'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»ŽÃ•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»žá» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä'
    
    @staticmethod
    def remove_html(text):
        return re.sub(r'<[^>]*>', '', text)
    
    @staticmethod
    def remove_emoji(text):
        return emoji.replace_emoji(text, ' ')
    
    @staticmethod
    def remove_url(text):
        return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
    
    @staticmethod
    def remove_email(text):
        return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
    
    @staticmethod
    def remove_phone_number(text):
        return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
    
    @staticmethod
    def remove_hashtags(text):
        return re.sub(r'#\w+', '', text)
    
    @staticmethod
    def remove_unnecessary_characters(text):
        text = re.sub(fr"[^\sa-zA-Z0-9{VietnameseTextCleaner.VN_CHARS}]", ' ', text)
        return re.sub(r'\s+', ' ', text).strip()
    
    @staticmethod
    def process_text(text):
        text = VietnameseTextCleaner.remove_html(text)
        text = VietnameseTextCleaner.remove_emoji(text)
        text = VietnameseTextCleaner.remove_url(text)
        text = VietnameseTextCleaner.remove_email(text)
        text = VietnameseTextCleaner.remove_phone_number(text)
        text = VietnameseTextCleaner.remove_hashtags(text)
        return VietnameseTextCleaner.remove_unnecessary_characters(text)
    
"""
Chuáº©n hÃ³a tá»« ngá»¯ teencodes vÃ  phÃ¢n Ä‘oáº¡n tá»« (word segment) sá»­ dá»¥ng VnCoreNLP.
"""
class VietnameseTextPreprocessor:
    def __init__(self, vncorenlp_dir='./VnCoreNLP', extra_teencodes=None):
        self.vncorenlp_dir = vncorenlp_dir
        self.extra_teencodes = extra_teencodes
        self._load_vncorenlp()
        self._build_teencodes()

    def _load_vncorenlp(self):
        jar_path = os.path.join(self.vncorenlp_dir, 'VnCoreNLP-1.2.jar')
        vocab_path = os.path.join(self.vncorenlp_dir, 'models', 'wordsegmenter', 'vi-vocab')
        rdr_path = os.path.join(self.vncorenlp_dir, 'models', 'wordsegmenter','wordsegmenter.rdr')

        missing = [p for p in (jar_path, vocab_path, rdr_path) if not os.path.exists(p)]
        if missing:
            raise FileNotFoundError(
                "Thiáº¿u file VnCoreNLP local. HÃ£y Ä‘áº·t sáºµn cÃ¡c file sau:\n"
                + "\n".join(f"- {p}" for p in (jar_path, vocab_path, rdr_path))
            )

        self.word_segmenter = VnCoreNLP(jar_path, annotators='wseg', quiet=False)
        print('VnCoreNLP word segmenter is loaded successfully.')
        
    def _build_teencodes(self):
        self.teencodes = {
            'ok': ['okie', 'okey', 'Ã´kÃª', 'oki', 'oke', 'okay', 'okÃª'], 
            'khÃ´ng': ['kg', 'not', 'k', 'kh', 'kÃ´', 'hok', 'ko', 'khong'], 'khÃ´ng pháº£i': ['kp'], 
            'cáº£m Æ¡n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'há»“i Ä‘Ã³': ['hÃ¹i Ä‘Ã³'], 'muá»‘n': ['mÃºn'],
            
            'ráº¥t tá»‘t': ['perfect', 'â¤ï¸', 'ðŸ˜'], 'dá»… thÆ°Æ¡ng': ['cute'], 'yÃªu': ['iu'], 'thÃ­ch': ['thik'], 
            'tá»‘t': [
                'gud', 'good', 'gÃºt', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)',
                'ðŸ‘', 'ðŸŽ‰', 'ðŸ˜€', 'ðŸ˜‚', 'ðŸ¤—', 'ðŸ˜™', 'ðŸ™‚'
            ], 
            'bÃ¬nh thÆ°á»ng': ['bt', 'bthg'], 'hÃ ng': ['hÃ g'], 
            'khÃ´ng tá»‘t':  ['lol', 'cc', 'huhu', ':(', 'ðŸ˜”', 'ðŸ˜“'],
            'tá»‡': ['sad', 'por', 'poor', 'bad'], 'giáº£ máº¡o': ['fake'], 
            
            'quÃ¡': ['wa', 'wÃ¡', 'qÃ¡'], 'Ä‘Æ°á»£c': ['Ä‘x', 'dk', 'dc', 'Ä‘k', 'Ä‘c'], 
            'vá»›i': ['vs'], 'gÃ¬': ['j'], 'rá»“i': ['r'], 'mÃ¬nh': ['m', 'mik'], 
            'thá»i gian': ['time'], 'giá»': ['h'], 
            
            'khÃ¡ch sáº¡n': ['ks'], 'nhÃ  hÃ ng': ['nhahang'], 'nhÃ¢n viÃªn': ['nv'],
            'cá»­a hÃ ng': ['store', 'sop', 'shopE', 'shop'], 
            'sáº£n pháº©m': ['sp', 'product'], 'hÃ ng': ['hÃ g'],
            'giao hÃ ng': ['ship', 'delivery', 'sÃ­p'], 'Ä‘áº·t hÃ ng': ['order'], 
            'chuáº©n chÃ­nh hÃ£ng': ['authentic', 'aut', 'auth'], 'háº¡n sá»­ dá»¥ng': ['date', 'hsd'],
            'Ä‘iá»‡n thoáº¡i': ['dt'],  'facebook': ['fb', 'face'],  
            'nháº¯n tin': ['nt', 'ib'], 'tráº£ lá»i': ['tl', 'trl', 'rep'], 
            'feedback': ['fback', 'fedback'], 'sá»­ dá»¥ng': ['sd'], 'xÃ i': ['sÃ i'],
        }
        if self.extra_teencodes: 
            for key, values in self.extra_teencodes.items():
                if any(len(value.split()) > 1 for value in values):
                    raise ValueError('The values for each key in extra_teencodes must be single words.')
                self.teencodes.setdefault(key, []).extend(values)

        # Äáº£o chiá»u: teencode -> vÄƒn báº£n chuáº©n
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
    
    def normalize_teencodes(self, text):
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    def word_segment(self, text):
        if self.word_segmenter: 
            words = self.word_segmenter.tokenize(text)
            return ' '.join(sum(words, [])) # Flatten the list of words
        print('There is no VnCoreNLP word segmenter loaded. Please check the VnCoreNLP jar file.')
        return text
    
    def process_text(self, text):
        text = text.lower()
        text = VietnameseToneNormalizer.normalize_unicode(text)
        text = VietnameseToneNormalizer.normalize_sentence_typing(text)
        text = self.normalize_teencodes(text)
        text = VietnameseTextCleaner.process_text(text)
        return self.word_segment(text)
    
    def preprocess_dataframe(self, df, text_col, out_col='review_clean'):
        s = df[text_col]
        cleaned = [self.process_text(x) for x in s]
        out = df.copy()
        out.insert(1, out_col, cleaned)
        out = out.drop(columns=[text_col])
        return out
    
    def close_vncorenlp(self):
        if self.word_segmenter: 
            print('Closing VnCoreNLP word segmenter...')
            self.word_segmenter.close()


class CustomPreprocessorTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, use_vncorenlp=False,vncorenlp_dir='./VnCoreNLP', extra_teencodes=None):
        self.use_vncorenlp = use_vncorenlp
        self.vncorenlp_dir = vncorenlp_dir
        self.extra_teencodes = extra_teencodes
        self.preprocessor = None
        
    def _ensure_pre(self):
        if self.use_vncorenlp and self.preprocessor is None:
            self.preprocessor = VietnameseTextPreprocessor(
                vncorenlp_dir=self.vncorenlp_dir,
                extra_teencodes=self.extra_teencodes
            )      
              
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        self._ensure_pre()
        s = pd.Series(X).fillna("").astype(str)

        if self.preprocessor is not None:
            result = s.map(self.preprocessor.process_text).tolist()
        else:
            raise ValueError("Preprocessor is not initialized.")
        
        self.close_vncorenlp()
        return result

    def close_vncorenlp(self):
        if self.preprocessor:
            self.preprocessor.close_vncorenlp()
            
    def __getstate__(self):
        state = self.__dict__.copy()
        state["preprocessor"] = None
        return state

    def __del__(self):
        self.close_vncorenlp()

if __name__ == '__main__':

    preprocessor = VietnameseTextPreprocessor(vncorenlp_dir='./processors/VnCoreNLP')
    
    dataset = ['train', 'val', 'test']
    
    for i in range(1, 4):
        df = pd.read_csv(f"D:/DS102/Data/{i}-{dataset[i-1]}.csv")
        df_clean = preprocessor.preprocess_dataframe(df, text_col='review', out_col='review_clean')
        # save to csv
        df_clean.to_csv(f"D:/DS102/Data/Preprocessed/{i}-{dataset[i-1]}-clean.csv", index=False)

    preprocessor.close_vncorenlp()
